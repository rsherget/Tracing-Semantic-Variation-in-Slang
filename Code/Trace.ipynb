{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supplementary code for paper submission: 'Tracing Semantic Variation in Slang'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the supplementary experiment code for 'Tracing Semantic Variation in Slang'. We provide our full model implementations and illustrate how experimental results in the paper can be reproduced using a small data sample (1 slang word from 3 dictionary entries) pre-processed by our provided data package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a list of non-standard Python packages you'll need. All of which can be obtained using *pip install*.\n",
    "\n",
    "- numpy\n",
    "- scipy\n",
    "- nltk\n",
    "- gensim\n",
    "- sklearn\n",
    "- torch\n",
    "- transformers\n",
    "- sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import pickle\n",
    "import re\n",
    "import urllib\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import scipy.spatial.distance as dist\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "from collections import defaultdict, namedtuple\n",
    "\n",
    "from nltk.corpus import stopwords as sw\n",
    "from gensim.utils import simple_preprocess\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from tqdm import trange\n",
    "\n",
    "from util import GSD_Definition, GSD_Word\n",
    "\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel\n",
    "\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining data structures and various helper functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Definition = namedtuple('Definition', ['word', 'type', 'def_sent', 'ex_sents', 'stamps'])\n",
    "\n",
    "re_hex = re.compile(r\"\\\\x[a-f0-9][a-f0-9]\")\n",
    "re_spacechar = re.compile(r\"\\\\(n|t)\")\n",
    "def proc_def(sent):\n",
    "    return re_spacechar.sub('', re_hex.sub('', sent))\n",
    "\n",
    "stopwords = set(sw.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_region(def_entry):\n",
    "    #############\n",
    "    # 0 - US\n",
    "    # 1 - UK\n",
    "    # 2 - Shared\n",
    "    #############\n",
    "            \n",
    "    stamp_set = set([s[1] for s in def_entry.stamps])\n",
    "    if '[US]' in stamp_set and '[UK]' in stamp_set:\n",
    "        return 2\n",
    "    elif '[US]' in stamp_set:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "def tag2str(tag):\n",
    "    if tag==0:\n",
    "        return '[US]'\n",
    "    if tag==1:\n",
    "        return '[UK]'\n",
    "    if tag==2:\n",
    "        return '[Shared]'\n",
    "\n",
    "def tags2str(tags):\n",
    "    results = []\n",
    "    for tag in tags:\n",
    "        results.append(tag2str(tag))\n",
    "    return results\n",
    "\n",
    "def normalize_pair(a, b, ep=0):\n",
    "    tmp = a+b+ep*2\n",
    "    return ((a+ep)/tmp, (b+ep)/tmp)\n",
    "\n",
    "def normalize_L2(array, axis=1):\n",
    "    if axis == 1:\n",
    "        return array / np.linalg.norm(array, axis=1)[:, np.newaxis]\n",
    "    if axis == 0:\n",
    "        return array / np.linalg.norm(array, axis=0)[np.newaxis, :]\n",
    "\n",
    "def SBERT_encode(model, sentences):\n",
    "    sbert_embeddings = np.asarray(model.encode(sentences))\n",
    "    return normalize_L2(sbert_embeddings, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the sample data processed by the provided data package. You can find both the pre-processing code and illustration in the provided notebook *Preprocess.ipynb* in the data package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide the final pre-processed result here for convenience. It contains 3 dictionary entries from Green's Dictionary of Slang (https://greensdictofslang.com/) all refering to the slang word *beast*. We filter out all definition entries that emerge before 1800 for our experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1360/1360 [00:00<00:00, 76263.13it/s]\n"
     ]
    }
   ],
   "source": [
    "data_GSD_raw = np.load('GSD_Data.npy', allow_pickle=True)\n",
    "\n",
    "GSD_by_word = defaultdict(list)\n",
    "\n",
    "for i in trange(data_GSD_raw.shape[0]):\n",
    "    entry = data_GSD_raw[i]\n",
    "    for d in entry.definitions:\n",
    "        def_sent_proc = proc_def(d.def_sent)\n",
    "        def_entry = Definition(entry.word, entry.pos, def_sent_proc, d.contexts, d.stamps)\n",
    "        if def_entry.stamps[0][0] < 1800:\n",
    "            continue\n",
    "        GSD_by_word[entry.word].append(def_entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code filters word entries based on the minimum number of regional senses a word has (i.e. the parameter k in our paper). It also collapses dictionary entries for the same word (i.e. homonyms) into single entries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the same time, we also filter out word entries with 1) no historical regional senses before 1900 and 2) no regional senses emerging after 1900 for prediction. We then extract all necessary data for those remaining entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_REGION = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "GSD_entries = defaultdict(list)\n",
    "GSD_regions = defaultdict(list)\n",
    "GSD_dates = defaultdict(list)\n",
    "\n",
    "GSD_entries_shared = defaultdict(list)\n",
    "\n",
    "for word in GSD_by_word.keys():\n",
    "    entries_all = GSD_by_word[word]\n",
    "    regions_all = []\n",
    "    \n",
    "    for entry in entries_all:\n",
    "        region = classify_region(entry)\n",
    "        regions_all.append(region)\n",
    "    regions_all = np.asarray(regions_all)\n",
    "    \n",
    "    entries = [entries_all[i] for i in range(len(entries_all)) if regions_all[i] != 2]\n",
    "    regions = regions_all[regions_all != 2]\n",
    "    \n",
    "    date_ind = np.argsort([entry.stamps[0][0] for entry in entries])\n",
    "    entries_sorted = [entries[i] for i in date_ind]\n",
    "    regions_sorted = regions[date_ind]\n",
    "    \n",
    "    dates = np.asarray([entry.stamps[0][0] for entry in entries_sorted])\n",
    "    if np.sum(dates < 1900) == 0:\n",
    "        continue\n",
    "    if np.sum(dates >= 1900) == 0:\n",
    "        continue\n",
    "    \n",
    "    if np.min([np.sum(regions==i) for i in range(2)]) >= MIN_REGION:\n",
    "        GSD_entries[word] = entries_sorted\n",
    "        GSD_regions[word] = regions_sorted\n",
    "        GSD_dates[word] = dates\n",
    "        \n",
    "        GSD_entries_shared[word] = [entries_all[i] for i in range(len(entries_all)) if regions_all[i] == 2]\n",
    "        \n",
    "exp_words = list(GSD_entries.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our sample data, we only have one word (i.e. *beast*) and it passes the filtering criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(exp_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['one',\n",
       " 'cracker',\n",
       " 'spiel',\n",
       " 'squeeze',\n",
       " 'gaff',\n",
       " 'flash',\n",
       " 'square',\n",
       " 'monkey',\n",
       " 'out',\n",
       " 'ticket',\n",
       " 'chop',\n",
       " 'jack',\n",
       " 'rush',\n",
       " 'lift',\n",
       " 'flag',\n",
       " 'rag',\n",
       " 'slum',\n",
       " 'pull',\n",
       " 'bird',\n",
       " 'nigger',\n",
       " 'pop',\n",
       " 'mug',\n",
       " 'split',\n",
       " 'smash',\n",
       " 'blue',\n",
       " 'soldier',\n",
       " 'stiff',\n",
       " 'shag',\n",
       " 'daddy',\n",
       " 'cop',\n",
       " 'jigger',\n",
       " 'crab',\n",
       " 'smoker',\n",
       " 'swag',\n",
       " 'banger',\n",
       " 'cow',\n",
       " 'wrong',\n",
       " 'flop',\n",
       " 'brown',\n",
       " 'stick',\n",
       " 'bitch',\n",
       " 'jag',\n",
       " 'plant',\n",
       " 'bit',\n",
       " 'snifter',\n",
       " 'jacket',\n",
       " 'long',\n",
       " 'dip',\n",
       " 'buster',\n",
       " 'crow',\n",
       " 'bag',\n",
       " 'kite',\n",
       " 'on',\n",
       " 'blow',\n",
       " 'bounce',\n",
       " 'red',\n",
       " 'joey',\n",
       " 'hit',\n",
       " 'skin',\n",
       " 'jump',\n",
       " 'drop',\n",
       " 'leg',\n",
       " 'tool',\n",
       " 'tiger',\n",
       " 'drag',\n",
       " 'crib',\n",
       " 'horse',\n",
       " 'jockey',\n",
       " 'bat',\n",
       " 'go',\n",
       " 'canary',\n",
       " 'burn',\n",
       " 'sticker',\n",
       " 'sweat',\n",
       " 'screw',\n",
       " 'thick',\n",
       " 'growler',\n",
       " 'pump',\n",
       " 'whistler',\n",
       " 'shoot',\n",
       " 'off',\n",
       " 'do',\n",
       " 'pill']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code automatically obtains normalized frequency estimates from Google Ngram. We provide an offline version of all the estimates we have collected but provide the code to illustrate the parameters used for querying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams_cache = pickle.load(open('ngrams_cache.pickle', 'rb'))\n",
    "\n",
    "def create_url(word, yr_start, yr_end, corpus='[US]', case_insensitive=True):\n",
    "    url = \"https://books.google.com/ngrams/graph?content=\"\n",
    "    url += word\n",
    "    if case_insensitive == True:\n",
    "        url += \"&case_insensitive=on\"\n",
    "    url += '&year_start='\n",
    "    url += str(yr_start)\n",
    "    url += '&year_end='\n",
    "    url += str(yr_end)\n",
    "    url += '&corpus='\n",
    "    if corpus == '[US]':\n",
    "        url += str(28)\n",
    "    if corpus == '[UK]':\n",
    "        url += str(29)\n",
    "    url += '&smoothing=0'\n",
    "    return url\n",
    "\n",
    "def url_query(word, yr_start, yr_end, corpus='[US]'):\n",
    "    results = 0\n",
    "    url = create_url(word, yr_start, yr_end, corpus)\n",
    "    try:\n",
    "        r = urllib.request.urlopen(url)\n",
    "        for line in str(r.read()).split('\\\\n'):\n",
    "            if 'ngrams-data' in line:\n",
    "                if len(line.strip().split(':')) >= 5:\n",
    "                    if 'ngram' in line.strip().split(':')[4][-6:]:\n",
    "                        results = [float(s.strip()) for s in line.strip().split(':')[4].strip()[1:-12].split(',')]\n",
    "                    else:\n",
    "                        results = [float(s.strip()) for s in line.strip().split(':')[4].strip()[1:-4].split(',')]\n",
    "                    break\n",
    "    except urllib.error.HTTPError as err:\n",
    "        if '429' in str(err):\n",
    "            return None\n",
    "        results = [0.]*10\n",
    "    time.sleep(2)\n",
    "    return results\n",
    "\n",
    "def ngram_lookup(word, year, corpus='[US]'):\n",
    "    if (word, year, corpus) not in ngrams_cache:\n",
    "        results = url_query(word, year-10, year-1, corpus)\n",
    "        while results is None:\n",
    "            print('Access Blocked - Waiting')\n",
    "            time.sleep(300)\n",
    "            results = url_query(word, year-10, year-1, corpus)\n",
    "        ngrams_cache[(word, year, corpus)] = np.mean(results)\n",
    "    return ngrams_cache[(word, year, corpus)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now run our models on the data. The following code will perform 20 trials as described in the paper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Trial 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 6/83 [03:37<47:44, 37.20s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Access Blocked - Waiting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 6/83 [08:53<1:54:01, 88.85s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 158\u001b[0m\n\u001b[1;32m    155\u001b[0m uk_more_freq \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    157\u001b[0m \u001b[39mfor\u001b[39;00m content_word \u001b[39min\u001b[39;00m content_words:\n\u001b[0;32m--> 158\u001b[0m     us_freq \u001b[39m=\u001b[39m ngram_lookup(content_word, date, \u001b[39m'\u001b[39;49m\u001b[39m[US]\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m    159\u001b[0m     uk_freq \u001b[39m=\u001b[39m ngram_lookup(content_word, date, \u001b[39m'\u001b[39m\u001b[39m[UK]\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    161\u001b[0m     us_freq_total \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m us_freq\n",
      "Cell \u001b[0;32mIn[40], line 45\u001b[0m, in \u001b[0;36mngram_lookup\u001b[0;34m(word, year, corpus)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[39mwhile\u001b[39;00m results \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     44\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mAccess Blocked - Waiting\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 45\u001b[0m     time\u001b[39m.\u001b[39;49msleep(\u001b[39m300\u001b[39;49m)\n\u001b[1;32m     46\u001b[0m     results \u001b[39m=\u001b[39m url_query(word, year\u001b[39m-\u001b[39m\u001b[39m10\u001b[39m, year\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, corpus)\n\u001b[1;32m     47\u001b[0m ngrams_cache[(word, year, corpus)] \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmean(results)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "N_trials = 20\n",
    "MEM = 30000\n",
    "categories = ['[US]', '[UK]']\n",
    "model_tags = ['sense_freq', 'sense_freq_shared', \\\n",
    "              'lda', 'lda_shared', 'logistic_reg', 'logistic_reg_shared', \\\n",
    "              '1nn', 'prototype', 'exemplar', 'exemplar_opt', \\\n",
    "              '1nn_shared', 'prototype_shared', 'exemplar_shared', 'exemplar_opt_shared']\n",
    "\n",
    "embedder = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "\n",
    "correct_counts_sample = {(category, n):defaultdict(int) for category in categories for n in range(N_trials)}\n",
    "trivial_counts_sample = {(category, n):0 for category in categories for n in range(N_trials)}\n",
    "pred_count_sample = {(category, n):0 for category in categories for n in range(N_trials)}\n",
    "\n",
    "for n in range(N_trials):\n",
    "    print(\"[Trial %d]\" % (n+1))\n",
    "    time.sleep(0.5)\n",
    "    for t in trange(len(exp_words)):\n",
    "        word = exp_words[t]\n",
    "\n",
    "        entries = GSD_entries[word]\n",
    "        regions = GSD_regions[word]\n",
    "        dates = GSD_dates[word]\n",
    "\n",
    "        def_sents = [entry.def_sent for entry in entries]\n",
    "        def_embeds = SBERT_encode(embedder, def_sents)\n",
    "        embed_dists = -1 * np.square(dist.cdist(def_embeds, def_embeds))\n",
    "\n",
    "        context_sents = []\n",
    "        for entry in entries:\n",
    "            if len(entry.ex_sents) > 0:\n",
    "                c = 0\n",
    "                while entry.stamps[c] not in entry.ex_sents:\n",
    "                    c += 1\n",
    "                context_sents.append(entry.ex_sents[entry.stamps[c]])\n",
    "            else:\n",
    "                context_sents.append(None)\n",
    "\n",
    "        entries_shared = GSD_entries_shared[word]\n",
    "\n",
    "        if len(entries_shared) > 0:\n",
    "            def_sents_shared = [entry.def_sent for entry in entries_shared]\n",
    "            def_embeds_shared = SBERT_encode(embedder, def_sents_shared)\n",
    "            embed_dists_shared = -1 * np.square(dist.cdist(def_embeds, def_embeds_shared))\n",
    "\n",
    "            us_shared_inds = []\n",
    "            uk_shared_inds = []\n",
    "\n",
    "            for i in range(len(entries)):\n",
    "\n",
    "                date = dates[i]\n",
    "\n",
    "                us_shared_pos = set()\n",
    "                uk_shared_pos = set()\n",
    "                for j in range(len(entries_shared)):\n",
    "                    seen_us = False\n",
    "                    seen_uk = False\n",
    "                    for stamp in entries_shared[j].stamps:\n",
    "                        if stamp[0] < date:\n",
    "                            if stamp[1] == '[US]' and not seen_us:\n",
    "                                seen_us = True\n",
    "                                if stamp[0] >= date-MEM:\n",
    "                                    us_shared_pos.add(j)\n",
    "                            if stamp[1] == '[UK]' and not seen_uk:\n",
    "                                seen_uk = True\n",
    "                                if stamp[0] >= date-MEM:\n",
    "                                    uk_shared_pos.add(j)\n",
    "                us_shared_inds.append(np.asarray(list(us_shared_pos)))\n",
    "                uk_shared_inds.append(np.asarray(list(uk_shared_pos)))\n",
    "                    \n",
    "        chain_memstart = []\n",
    "        for i in range(len(dates)):\n",
    "            memstart = 0\n",
    "            while memstart < i:\n",
    "                if dates[memstart] >= dates[i]-MEM:\n",
    "                    break\n",
    "                memstart += 1\n",
    "            chain_memstart.append(memstart)\n",
    "\n",
    "        exemplar_valid_pos = []\n",
    "        for i in range(len(entries)):\n",
    "            observed = np.asarray([False, False])\n",
    "            for j in range(chain_memstart[i], i):\n",
    "                observed[regions[j]] = True\n",
    "            if np.all(observed):\n",
    "                exemplar_valid_pos.append(i)\n",
    "        exemplar_valid_pos = np.asarray(exemplar_valid_pos, dtype=np.int32)\n",
    "\n",
    "        h_old = defaultdict(lambda:1)\n",
    "\n",
    "        priors = {'semantic_freq':[(0.5,0.5)], 'semantic_major':[(0.5,0.5)], \\\n",
    "                  'context_freq':[(0.5,0.5)], 'context_major':[(0.5,0.5)], \\\n",
    "                  'form_need':[(0.5,0.5)]}\n",
    "\n",
    "        for p in range(1, len(dates)):\n",
    "\n",
    "            example_regions = regions[:p]\n",
    "\n",
    "            if dates[p] < 1900:\n",
    "                for key in priors.keys():\n",
    "                    priors[key].append((0.5,0.5))\n",
    "                continue\n",
    "\n",
    "            def_sent = def_sents[p]\n",
    "            date = dates[p]\n",
    "            \n",
    "            # Form Need\n",
    "\n",
    "            us_slang_freq = ngram_lookup(word, date, '[US]')\n",
    "            uk_slang_freq = ngram_lookup(word, date, '[UK]')\n",
    "\n",
    "            priors['form_need'].append(normalize_pair(us_slang_freq, uk_slang_freq, ep=1e-8))\n",
    "\n",
    "            # Semantic Need\n",
    "\n",
    "            content_words = [w for w in simple_preprocess(def_sent) if w not in stopwords]\n",
    "\n",
    "            us_freq_total = 0\n",
    "            uk_freq_total = 0\n",
    "\n",
    "            us_more_freq = 0\n",
    "            uk_more_freq = 0\n",
    "\n",
    "            for content_word in content_words:\n",
    "                us_freq = ngram_lookup(content_word, date, '[US]')\n",
    "                uk_freq = ngram_lookup(content_word, date, '[UK]')\n",
    "\n",
    "                us_freq_total += us_freq\n",
    "                uk_freq_total += uk_freq\n",
    "\n",
    "                if uk_freq > us_freq:\n",
    "                    uk_more_freq += 1\n",
    "                else:\n",
    "                    us_more_freq += 1\n",
    "\n",
    "            priors['semantic_freq'].append(normalize_pair(us_freq_total, uk_freq_total, ep=1e-8))\n",
    "            priors['semantic_major'].append(normalize_pair(us_more_freq, uk_more_freq, ep=1))\n",
    "\n",
    "            # Context Need\n",
    "\n",
    "            if context_sents[p] is None:\n",
    "                priors['context_freq'].append((0.5, 0.5))\n",
    "                priors['context_major'].append((0.5, 0.5))\n",
    "            else:\n",
    "\n",
    "                context_sent = context_sents[p]\n",
    "                date = dates[p]\n",
    "\n",
    "                content_words = [w for w in simple_preprocess(context_sent) if (w not in stopwords and w != word)]\n",
    "\n",
    "                us_freq_total = 0\n",
    "                uk_freq_total = 0\n",
    "\n",
    "                us_more_freq = 0\n",
    "                uk_more_freq = 0\n",
    "\n",
    "                for content_word in content_words:\n",
    "                    us_freq = ngram_lookup(content_word, date, '[US]')\n",
    "                    uk_freq = ngram_lookup(content_word, date, '[UK]')\n",
    "\n",
    "                    us_freq_total += us_freq\n",
    "                    uk_freq_total += uk_freq\n",
    "\n",
    "                    if uk_freq > us_freq:\n",
    "                        uk_more_freq += 1\n",
    "                    else:\n",
    "                        us_more_freq += 1\n",
    "\n",
    "                priors['context_freq'].append(normalize_pair(us_freq_total, uk_freq_total, ep=1e-8))\n",
    "                priors['context_major'].append(normalize_pair(us_more_freq, uk_more_freq, ep=1))\n",
    "\n",
    "        # Sample test senses\n",
    "        \n",
    "        chain_start = 1\n",
    "        while dates[chain_start] < 1900:\n",
    "            chain_start += 1\n",
    "\n",
    "        chain_us = np.arange(chain_start, len(dates))[regions[chain_start:]==0]\n",
    "        chain_uk = np.arange(chain_start, len(dates))[regions[chain_start:]==1]\n",
    "\n",
    "        N_sample = min(len(chain_us), len(chain_uk))\n",
    "        if N_sample == 0:\n",
    "            continue\n",
    "\n",
    "        if len(chain_us) > N_sample:\n",
    "            chain_us = chain_us[np.random.choice(len(chain_us), N_sample, replace=False)]\n",
    "        if len(chain_uk) > N_sample:\n",
    "            chain_uk = chain_uk[np.random.choice(len(chain_uk), N_sample, replace=False)]\n",
    "\n",
    "        chain = np.sort(np.concatenate((chain_us, chain_uk)))\n",
    "            \n",
    "        for chain_pos in chain:\n",
    "\n",
    "            if dates[chain_pos] < 1900:\n",
    "                continue\n",
    "\n",
    "            example_regions = regions[chain_memstart[chain_pos]:chain_pos]\n",
    "            target_region = regions[chain_pos]\n",
    "            target_str = tag2str(target_region)\n",
    "\n",
    "            preds = defaultdict(int)\n",
    "            pred_count_sample[(target_str, n)] += 1\n",
    "\n",
    "            # Communicative Need Models\n",
    "\n",
    "            preds['form_need'] = priors['form_need'][chain_pos][0] < priors['form_need'][chain_pos][1]\n",
    "\n",
    "            preds['semantic_freq'] = priors['semantic_freq'][chain_pos][0] < priors['semantic_freq'][chain_pos][1]\n",
    "            preds['semantic_major'] = priors['semantic_major'][chain_pos][0] < priors['semantic_major'][chain_pos][1]\n",
    "\n",
    "            preds['context_freq'] = priors['context_freq'][chain_pos][0] < priors['context_freq'][chain_pos][1]\n",
    "            preds['context_major'] = priors['context_major'][chain_pos][0] < priors['context_major'][chain_pos][1]\n",
    "\n",
    "            skip_trivial = False\n",
    "\n",
    "            if np.sum(example_regions==0) == 0:\n",
    "                for m_tag in model_tags:\n",
    "                    preds[m_tag] = 1\n",
    "                skip_trivial = True\n",
    "\n",
    "            if np.sum(example_regions==1) == 0:\n",
    "                for m_tag in model_tags:\n",
    "                    preds[m_tag] = 0\n",
    "                skip_trivial = True\n",
    "\n",
    "            if skip_trivial:\n",
    "                for key, value in preds.items():\n",
    "                    if value == target_region:\n",
    "                        correct_counts_sample[(target_str, n)][key] += 1\n",
    "                continue\n",
    "\n",
    "            # Sense Frequency\n",
    "\n",
    "            preds['sense_freq'] = np.sum(example_regions==0) < np.sum(example_regions==1)\n",
    "            if len(entries_shared) > 0:\n",
    "                preds['sense_freq_shared'] = (np.sum(example_regions==0) + len(us_shared_inds[chain_pos])) < \\\n",
    "                                             (np.sum(example_regions==1) + len(uk_shared_inds[chain_pos]))\n",
    "            else:\n",
    "                preds['sense_freq_shared'] = preds['sense_freq']\n",
    "\n",
    "            # LDA method\n",
    "\n",
    "            if chain_pos - chain_memstart[chain_pos] == 2:\n",
    "                # Need more examples than number of classes\n",
    "                preds['lda'] = 0\n",
    "                preds['lda_shared'] = 0\n",
    "            else:\n",
    "                lda = LinearDiscriminantAnalysis(n_components=1)\n",
    "                \n",
    "                lda.fit(def_embeds[chain_memstart[chain_pos]:chain_pos], example_regions)\n",
    "                preds['lda'] = lda.predict(def_embeds[chain_pos][np.newaxis, :])\n",
    "\n",
    "                if len(entries_shared) == 0:\n",
    "                    preds['lda_shared'] = preds['lda']\n",
    "                else:\n",
    "                    us_shared_pos = [i for i in us_shared_inds[chain_pos] if i not in uk_shared_inds[chain_pos]]\n",
    "                    uk_shared_pos = [i for i in uk_shared_inds[chain_pos] if i not in us_shared_inds[chain_pos]]\n",
    "\n",
    "                    embeds_tmp = np.concatenate((def_embeds[chain_memstart[chain_pos]:chain_pos], def_embeds_shared[us_shared_pos], def_embeds_shared[uk_shared_pos]), axis=0)\n",
    "                    regions_tmp = np.concatenate((example_regions, [0]*len(us_shared_pos), [1]*len(uk_shared_pos)))\n",
    "\n",
    "                    lda_shared = LinearDiscriminantAnalysis(n_components=1)\n",
    "                    lda_shared.fit(embeds_tmp, regions_tmp)\n",
    "                    preds['lda_shared'] = lda_shared.predict(def_embeds[chain_pos][np.newaxis, :])\n",
    "\n",
    "            # Logistic Regression\n",
    "\n",
    "            lr = LogisticRegression().fit(def_embeds[chain_memstart[chain_pos]:chain_pos], example_regions)\n",
    "            preds['logistic_reg'] = lr.predict(def_embeds[chain_pos][np.newaxis, :])\n",
    "\n",
    "            if len(entries_shared) == 0:\n",
    "                preds['logistic_reg_shared'] = preds['logistic_reg']\n",
    "            else:\n",
    "                lr_shared = LogisticRegression().fit(embeds_tmp, regions_tmp)\n",
    "                preds['logistic_reg_shared'] = lr_shared.predict(def_embeds[chain_pos][np.newaxis, :])\n",
    "\n",
    "            # Semantic Chaining\n",
    "\n",
    "            us_dists = np.exp(embed_dists[chain_pos][chain_memstart[chain_pos]:chain_pos][example_regions==0])\n",
    "            uk_dists = np.exp(embed_dists[chain_pos][chain_memstart[chain_pos]:chain_pos][example_regions==1])\n",
    "\n",
    "            us_def_embeds = def_embeds[chain_memstart[chain_pos]:chain_pos][example_regions==0]\n",
    "            uk_def_embeds = def_embeds[chain_memstart[chain_pos]:chain_pos][example_regions==1]\n",
    "\n",
    "            us_prototype = np.mean(us_def_embeds, axis=0)\n",
    "            uk_prototype = np.mean(uk_def_embeds, axis=0)\n",
    "\n",
    "            us_proto_dist = np.linalg.norm(us_prototype-def_embeds[chain_pos])\n",
    "            uk_proto_dist = np.linalg.norm(uk_prototype-def_embeds[chain_pos])\n",
    "\n",
    "            preds['1nn'] = int(np.max(us_dists) < np.max(uk_dists))\n",
    "            preds['exemplar'] = int(np.mean(us_dists) < np.mean(uk_dists))\n",
    "            preds['prototype'] = int(us_proto_dist > uk_proto_dist)\n",
    "\n",
    "            # Optimize kernel width parameter (h) if training data is available\n",
    "            # Only need to optimize exemplar since we're not using a prior\n",
    "\n",
    "            exemplar_starts = exemplar_valid_pos[exemplar_valid_pos < chain_pos]\n",
    "            if exemplar_starts.shape[0] != 0:\n",
    "\n",
    "                def compute_exemplar_nll(h=1):\n",
    "                    nll = 0\n",
    "\n",
    "                    for pred_pos in exemplar_starts:\n",
    "                            \n",
    "                        region_sub = regions[chain_memstart[pred_pos]:pred_pos]\n",
    "\n",
    "                        us_dists = np.exp(embed_dists[pred_pos][chain_memstart[pred_pos]:pred_pos][region_sub==0] / h)\n",
    "                        uk_dists = np.exp(embed_dists[pred_pos][chain_memstart[pred_pos]:pred_pos][region_sub==1] / h)\n",
    "\n",
    "                        pred_dist = np.asarray([np.mean(us_dists), np.mean(uk_dists)])\n",
    "                        pred_dist = pred_dist / np.sum(pred_dist)\n",
    "                        nll += np.log(pred_dist[regions[pred_pos]])\n",
    "                    return -1 * nll\n",
    "\n",
    "                results = minimize(compute_exemplar_nll, [h_old['exemplar']], bounds=((10**-2, 10**2),))\n",
    "                h_old['exemplar'] = results.x[0]\n",
    "\n",
    "                us_dists = np.exp(embed_dists[chain_pos][chain_memstart[chain_pos]:chain_pos][example_regions==0] / h_old['exemplar_opt'])\n",
    "                uk_dists = np.exp(embed_dists[chain_pos][chain_memstart[chain_pos]:chain_pos][example_regions==1] / h_old['exemplar_opt'])\n",
    "\n",
    "                preds['exemplar'] = int(np.mean(us_dists) < np.mean(uk_dists))\n",
    "\n",
    "                \n",
    "            # Chaining with shared senses\n",
    "\n",
    "            if len(entries_shared) == 0:\n",
    "                preds['1nn_shared'] = preds['1nn']\n",
    "                preds['exemplar_shared'] = preds['exemplar']\n",
    "                preds['prototype_shared'] = preds['prototype']\n",
    "            else:\n",
    "                us_shared_pos = us_shared_inds[chain_pos]\n",
    "                uk_shared_pos = uk_shared_inds[chain_pos]\n",
    "\n",
    "                if us_shared_pos.shape[0] > 0:\n",
    "                    us_dists = np.exp(np.concatenate((embed_dists[chain_pos][chain_memstart[chain_pos]:chain_pos][example_regions==0], embed_dists_shared[chain_pos][us_shared_pos])))\n",
    "                    us_def_embeds = np.concatenate((def_embeds[chain_memstart[chain_pos]:chain_pos][example_regions==0], def_embeds_shared[us_shared_pos]), axis=0)\n",
    "                else:\n",
    "                    us_dists = np.exp(embed_dists[chain_pos][chain_memstart[chain_pos]:chain_pos][example_regions==0])\n",
    "                    us_def_embeds = def_embeds[chain_memstart[chain_pos]:chain_pos][example_regions==0]\n",
    "\n",
    "                if uk_shared_pos.shape[0] > 0:\n",
    "                    uk_dists = np.exp(np.concatenate((embed_dists[chain_pos][chain_memstart[chain_pos]:chain_pos][example_regions==1], embed_dists_shared[chain_pos][uk_shared_pos])))\n",
    "                    uk_def_embeds = np.concatenate((def_embeds[chain_memstart[chain_pos]:chain_pos][example_regions==1], def_embeds_shared[uk_shared_pos]), axis=0)\n",
    "                else:\n",
    "                    uk_dists = np.exp(embed_dists[chain_pos][chain_memstart[chain_pos]:chain_pos][example_regions==1])\n",
    "                    uk_def_embeds = def_embeds[chain_memstart[chain_pos]:chain_pos][example_regions==1]\n",
    "\n",
    "                us_prototype = np.mean(us_def_embeds, axis=0)\n",
    "                uk_prototype = np.mean(uk_def_embeds, axis=0)\n",
    "\n",
    "                us_proto_dist = np.linalg.norm(us_prototype-def_embeds[chain_pos])\n",
    "                uk_proto_dist = np.linalg.norm(uk_prototype-def_embeds[chain_pos])\n",
    "\n",
    "                preds['1nn_shared'] = int(np.max(us_dists) < np.max(uk_dists))\n",
    "                preds['exemplar_shared'] = int(np.mean(us_dists) < np.mean(uk_dists))\n",
    "                preds['prototype_shared'] = int(us_proto_dist > uk_proto_dist)\n",
    "\n",
    "                exemplar_starts = exemplar_valid_pos[exemplar_valid_pos < chain_pos]\n",
    "                if exemplar_starts.shape[0] != 0:\n",
    "\n",
    "                    def compute_exemplar_nll(h=1):\n",
    "                        nll = 0\n",
    "\n",
    "                        for pred_pos in exemplar_starts:\n",
    "                            region_sub = regions[chain_memstart[pred_pos]:pred_pos]\n",
    "\n",
    "                            us_shared_pos = us_shared_inds[pred_pos]\n",
    "                            uk_shared_pos = uk_shared_inds[pred_pos]\n",
    "\n",
    "                            if us_shared_pos.shape[0] > 0:\n",
    "                                us_dists = np.exp(np.concatenate((embed_dists[pred_pos][chain_memstart[pred_pos]:pred_pos][region_sub==0], embed_dists_shared[pred_pos][us_shared_pos])) / h)\n",
    "                            else:\n",
    "                                us_dists = np.exp(embed_dists[pred_pos][chain_memstart[pred_pos]:pred_pos][region_sub==0] / h)\n",
    "\n",
    "                            if uk_shared_pos.shape[0] > 0:\n",
    "                                uk_dists = np.exp(np.concatenate((embed_dists[pred_pos][chain_memstart[pred_pos]:pred_pos][region_sub==1], embed_dists_shared[pred_pos][uk_shared_pos])) / h)\n",
    "                            else:\n",
    "                                uk_dists = np.exp(embed_dists[pred_pos][chain_memstart[pred_pos]:pred_pos][region_sub==1] / h)\n",
    "\n",
    "                            pred_dist = np.asarray([np.mean(us_dists), np.mean(uk_dists)])\n",
    "                            pred_dist = pred_dist / np.sum(pred_dist)\n",
    "                            nll += np.log(pred_dist[regions[pred_pos]])\n",
    "                        return -1 * nll\n",
    "\n",
    "                    results = minimize(compute_exemplar_nll, [h_old['exemplar_shared']], bounds=((10**-2, 10**2),))\n",
    "                    h_old['exemplar_shared'] = results.x[0]\n",
    "\n",
    "                    us_shared_pos = us_shared_inds[chain_pos]\n",
    "                    uk_shared_pos = uk_shared_inds[chain_pos]\n",
    "\n",
    "                    if us_shared_pos.shape[0] > 0:\n",
    "                        us_dists = np.exp(np.concatenate((embed_dists[chain_pos][chain_memstart[chain_pos]:chain_pos][example_regions==0], embed_dists_shared[chain_pos][us_shared_pos])) / h_old['exemplar_opt_shared'])\n",
    "                    else:\n",
    "                        us_dists = np.exp(embed_dists[chain_pos][chain_memstart[chain_pos]:chain_pos][example_regions==0] / h_old['exemplar_opt_shared'])\n",
    "\n",
    "                    if uk_shared_pos.shape[0] > 0:\n",
    "                        uk_dists = np.exp(np.concatenate((embed_dists[chain_pos][chain_memstart[chain_pos]:chain_pos][example_regions==1], embed_dists_shared[chain_pos][uk_shared_pos])) / h_old['exemplar_opt_shared'])\n",
    "                    else:\n",
    "                        uk_dists = np.exp(embed_dists[chain_pos][chain_memstart[chain_pos]:chain_pos][example_regions==1] / h_old['exemplar_opt_shared'])\n",
    "\n",
    "                    preds['exemplar_shared'] = int(np.mean(us_dists) < np.mean(uk_dists))\n",
    "\n",
    "               \n",
    "            # Collect Results\n",
    "            for key, value in preds.items():\n",
    "                if value == target_region:\n",
    "                    correct_counts_sample[(target_str, n)][key] += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the mean predictive accuracy (measured in percentage, with standard deviation in paranthesis) on entries corresponding to the word *beast*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               [US]        [UK]         Total\n",
      "[BASELINE]\n",
      "                         SENSE_FREQ:   86.7 (16.33)  0.0 (0.00)  43.3 (8.16)\n",
      "                  SENSE_FREQ_SHARED:   86.7 (16.33)  0.0 (0.00)  43.3 (8.16)\n",
      "\n",
      "[NEED]\n",
      "                          FORM_NEED:   15.0 (16.58)  100.0 (0.00)  57.5 (8.29)\n",
      "                      SEMANTIC_FREQ:   35.0 (24.66)  0.0 (0.00)  17.5 (12.33)\n",
      "                     SEMANTIC_MAJOR:   66.7 (23.57)  0.0 (0.00)  33.3 (11.79)\n",
      "                       CONTEXT_FREQ:   43.3 (23.80)  0.0 (0.00)  21.7 (11.90)\n",
      "                      CONTEXT_MAJOR:   43.3 (23.80)  0.0 (0.00)  21.7 (11.90)\n",
      "\n",
      "[SIMPLE]\n",
      "                                LDA:   83.3 (26.87)  33.3 (0.00)  58.3 (13.44)\n",
      "                         LDA_SHARED:   83.3 (26.87)  33.3 (0.00)  58.3 (13.44)\n",
      "                       LOGISTIC_REG:   86.7 (16.33)  0.0 (0.00)  43.3 (8.16)\n",
      "                LOGISTIC_REG_SHARED:   86.7 (16.33)  0.0 (0.00)  43.3 (8.16)\n",
      "\n",
      "[CHAINING]\n",
      "                                1NN:   91.7 (14.43)  66.7 (0.00)  79.2 (7.22)\n",
      "                          PROTOTYPE:   83.3 (19.72)  66.7 (0.00)  75.0 (9.86)\n",
      "                           EXEMPLAR:   83.3 (19.72)  66.7 (0.00)  75.0 (9.86)\n",
      "\n",
      "[CHAINING - SHARED]\n",
      "                         1NN_SHARED:   91.7 (14.43)  66.7 (0.00)  79.2 (7.22)\n",
      "                   PROTOTYPE_SHARED:   83.3 (19.72)  66.7 (0.00)  75.0 (9.86)\n",
      "                    EXEMPLAR_SHARED:   83.3 (19.72)  66.7 (0.00)  75.0 (9.86)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_list = {'Baseline': ['sense_freq', 'sense_freq_shared'],\\\n",
    "                      'Need':['form_need', 'semantic_freq', 'semantic_major', 'context_freq', 'context_major'],\\\n",
    "                      'Simple':['lda', 'lda_shared', 'logistic_reg', 'logistic_reg_shared'],\\\n",
    "                      'Chaining':['1nn', 'prototype', 'exemplar'],\\\n",
    "                      'Chaining - Shared':['1nn_shared', 'prototype_shared', 'exemplar_shared']}\n",
    "\n",
    "print(\"%51s%12s%14s\" % (\"[US]\", \"[UK]\", \"Total\"))\n",
    "for group, models in model_list.items():\n",
    "    print(\"[\"+group.upper()+\"]\")\n",
    "    for model in models:\n",
    "\n",
    "        us_correct = np.asarray([correct_counts_sample[('[US]', n)][model] for n in range(N_trials)])\n",
    "        us_total = np.asarray([pred_count_sample[('[US]', n)] for n in range(N_trials)])\n",
    "\n",
    "        uk_correct = np.asarray([correct_counts_sample[('[UK]', n)][model] for n in range(N_trials)])\n",
    "        uk_total = np.asarray([pred_count_sample[('[UK]', n)] for n in range(N_trials)])\n",
    "\n",
    "        print(\"%35s:   %.1f (%.2f)  %.1f (%.2f)  %.1f (%.2f)\" % \\\n",
    "            (model.upper(), \\\n",
    "            np.mean(us_correct / us_total * 100), np.std(us_correct / us_total * 100),\\\n",
    "            np.mean(uk_correct / uk_total * 100), np.std(uk_correct / uk_total * 100),\\\n",
    "            np.mean((us_correct + uk_correct) / (us_total+uk_total) * 100), np.std((us_correct + uk_correct) / (us_total+uk_total) * 100)))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "7e1998ff7f8aa20ada591c520b972326324e5ea05489af9e422744c7c09f6dad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
